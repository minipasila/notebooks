{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u223xpyHk5Jw"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers torch torchaudio librosa datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_O_4WzFxicD2"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
        "import librosa\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Wav2VecInterface:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.processor = None\n",
        "        self.feature_extractor = None\n",
        "        self.current_model_name = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def load_model(self, model_url_or_name):\n",
        "        try:\n",
        "            # Extract model name from URL if it's a full URL\n",
        "            if \"huggingface.co\" in model_url_or_name:\n",
        "                model_name = re.search(r'huggingface\\.co/([^/]+/[^/?]+)', model_url_or_name)\n",
        "                if model_name:\n",
        "                    model_name = model_name.group(1)\n",
        "                else:\n",
        "                    return \"‚ùå Invalid Hugging Face URL format\", None, None\n",
        "            else:\n",
        "                model_name = model_url_or_name\n",
        "\n",
        "            # Check if model is already loaded\n",
        "            if self.current_model_name == model_name:\n",
        "                return f\"‚úÖ Model {model_name} is already loaded\", None, None\n",
        "\n",
        "            # Load the model and processor\n",
        "            try:\n",
        "                self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "                self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            except:\n",
        "                # Fallback to feature extractor only if processor fails\n",
        "                self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "                self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "                self.processor = None\n",
        "\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            self.current_model_name = model_name\n",
        "\n",
        "            # Get model info\n",
        "            model_info = f\"\"\"\n",
        "**Model Information:**\n",
        "- **Name:** {model_name}\n",
        "- **Device:** {self.device}\n",
        "- **Model Type:** Wav2Vec2ForCTC\n",
        "- **Parameters:** {sum(p.numel() for p in self.model.parameters()):,}\n",
        "- **Processor Available:** {'Yes' if self.processor else 'No (using feature extractor only)'}\n",
        "            \"\"\"\n",
        "\n",
        "            return f\"‚úÖ Successfully loaded model: {model_name}\", model_info, gr.update(interactive=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading model: {str(e)}\", None, None\n",
        "\n",
        "    def transcribe_audio(self, audio_file):\n",
        "        if self.model is None:\n",
        "            return \"‚ùå Please load a model first!\"\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            if audio_file is None:\n",
        "                return \"‚ùå Please upload an audio file!\"\n",
        "\n",
        "            # Load audio file\n",
        "            audio_input, sample_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "            # Prepare input\n",
        "            if self.processor:\n",
        "                input_values = self.processor(\n",
        "                    audio_input,\n",
        "                    sampling_rate=16000,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "            else:\n",
        "                input_values = self.feature_extractor(\n",
        "                    audio_input,\n",
        "                    sampling_rate=16000,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "\n",
        "            # Move to device\n",
        "            input_values = input_values.to(self.device)\n",
        "\n",
        "            # Inference\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(input_values).logits\n",
        "\n",
        "            # Get predicted IDs\n",
        "            predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Decode transcription\n",
        "            if self.processor:\n",
        "                transcription = self.processor.decode(predicted_ids[0])\n",
        "            else:\n",
        "                # Fallback decoding (may not work for all models)\n",
        "                transcription = \"Transcription unavailable - processor required for decoding\"\n",
        "\n",
        "            # Audio info\n",
        "            duration = len(audio_input) / 16000\n",
        "            audio_info = f\"\"\"\n",
        "**Audio Information:**\n",
        "- **Duration:** {duration:.2f} seconds\n",
        "- **Sample Rate:** 16000 Hz (resampled)\n",
        "- **Shape:** {audio_input.shape}\n",
        "            \"\"\"\n",
        "\n",
        "            return transcription, audio_info\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error during transcription: {str(e)}\", None\n",
        "\n",
        "    def extract_features(self, audio_file):\n",
        "        if self.model is None:\n",
        "            return \"‚ùå Please load a model first!\"\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            if audio_file is None:\n",
        "                return \"‚ùå Please upload an audio file!\"\n",
        "\n",
        "            # Load audio file\n",
        "            audio_input, sample_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "            # Prepare input\n",
        "            if self.processor:\n",
        "                input_values = self.processor(\n",
        "                    audio_input,\n",
        "                    sampling_rate=16000,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "            else:\n",
        "                input_values = self.feature_extractor(\n",
        "                    audio_input,\n",
        "                    sampling_rate=16000,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "\n",
        "            # Move to device\n",
        "            input_values = input_values.to(self.device)\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                # Get hidden states from the model\n",
        "                outputs = self.model.wav2vec2(input_values)\n",
        "                last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "            # Feature info\n",
        "            features_info = f\"\"\"\n",
        "**Feature Information:**\n",
        "- **Feature Shape:** {last_hidden_state.shape}\n",
        "- **Hidden Size:** {last_hidden_state.shape[-1]}\n",
        "- **Sequence Length:** {last_hidden_state.shape[1]}\n",
        "- **Mean Activation:** {last_hidden_state.mean().item():.6f}\n",
        "- **Std Activation:** {last_hidden_state.std().item():.6f}\n",
        "            \"\"\"\n",
        "\n",
        "            return features_info\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error during feature extraction: {str(e)}\"\n",
        "\n",
        "# Initialize the interface\n",
        "wav2vec_interface = Wav2VecInterface()\n",
        "\n",
        "# Popular Wav2Vec2 models for quick selection\n",
        "popular_models = [\n",
        "    \"facebook/wav2vec2-base-960h\",\n",
        "    \"facebook/wav2vec2-large-960h\",\n",
        "    \"facebook/wav2vec2-base-100h\",\n",
        "    \"facebook/wav2vec2-large-960h-lv60-self\",\n",
        "    \"jonatasgrosman/wav2vec2-large-xlsr-53-english\",\n",
        "    \"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\",\n",
        "    \"jonatasgrosman/wav2vec2-large-xlsr-53-french\",\n",
        "    \"jonatasgrosman/wav2vec2-large-xlsr-53-german\",\n",
        "    \"wavlm/wavlm-base\",\n",
        "    \"microsoft/unispeech-sat-base\",\n",
        "]\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Wav2Vec2 Model Interface\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # üéµ Wav2Vec2 Model Interface\n",
        "\n",
        "            Load and test Wav2Vec2 models from Hugging Face for speech recognition and feature extraction.\n",
        "\n",
        "            **Instructions:**\n",
        "            1. Enter a Hugging Face model URL or model name (e.g., `facebook/wav2vec2-base-960h`)\n",
        "            2. Click \"Load Model\" to download and initialize the model\n",
        "            3. Upload an audio file and choose your task (transcription or feature extraction)\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                model_input = gr.Textbox(\n",
        "                    label=\"ü§ó Hugging Face Model URL or Name\",\n",
        "                    placeholder=\"e.g., facebook/wav2vec2-base-960h or https://huggingface.co/facebook/wav2vec2-base-960h\",\n",
        "                    value=\"facebook/wav2vec2-base-960h\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    load_btn = gr.Button(\"üîÑ Load Model\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"üóëÔ∏è Clear\", variant=\"secondary\")\n",
        "\n",
        "                popular_dropdown = gr.Dropdown(\n",
        "                    label=\"üåü Popular Models (Quick Select)\",\n",
        "                    choices=popular_models,\n",
        "                    value=None,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"üìä Status\",\n",
        "                    interactive=False,\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "        model_info_output = gr.Markdown(label=\"‚ÑπÔ∏è Model Information\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                audio_input = gr.Audio(\n",
        "                    label=\"üé§ Upload Audio File\",\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    transcribe_btn = gr.Button(\n",
        "                        \"üî§ Transcribe\",\n",
        "                        variant=\"primary\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "                    features_btn = gr.Button(\n",
        "                        \"üß† Extract Features\",\n",
        "                        variant=\"secondary\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            with gr.Column():\n",
        "                transcription_output = gr.Textbox(\n",
        "                    label=\"üìù Transcription\",\n",
        "                    lines=3,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                audio_info_output = gr.Markdown(label=\"üéµ Audio Information\")\n",
        "                features_output = gr.Markdown(label=\"üîç Features Information\")\n",
        "\n",
        "        # Event handlers\n",
        "        def load_model_handler(model_name):\n",
        "            status, info, btn_update = wav2vec_interface.load_model(model_name)\n",
        "            if btn_update is not None:\n",
        "                return status, info, btn_update, btn_update\n",
        "            return status, info, gr.update(), gr.update()\n",
        "\n",
        "        def clear_handler():\n",
        "            wav2vec_interface.model = None\n",
        "            wav2vec_interface.processor = None\n",
        "            wav2vec_interface.current_model_name = None\n",
        "            return (\n",
        "                \"\",  # model_input\n",
        "                \"üóëÔ∏è Cleared\", # status\n",
        "                \"\",  # model_info\n",
        "                \"\",  # transcription\n",
        "                \"\",  # audio_info\n",
        "                \"\",  # features\n",
        "                gr.update(interactive=False),  # transcribe_btn\n",
        "                gr.update(interactive=False),  # features_btn\n",
        "            )\n",
        "\n",
        "        def transcribe_handler(audio):\n",
        "            transcription, audio_info = wav2vec_interface.transcribe_audio(audio)\n",
        "            return transcription, audio_info\n",
        "\n",
        "        def features_handler(audio):\n",
        "            features_info = wav2vec_interface.extract_features(audio)\n",
        "            return features_info\n",
        "\n",
        "        def popular_model_selected(choice):\n",
        "            if choice:\n",
        "                return choice\n",
        "            return gr.update()\n",
        "\n",
        "        # Connect events\n",
        "        load_btn.click(\n",
        "            load_model_handler,\n",
        "            inputs=[model_input],\n",
        "            outputs=[status_output, model_info_output, transcribe_btn, features_btn]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_handler,\n",
        "            outputs=[\n",
        "                model_input, status_output, model_info_output,\n",
        "                transcription_output, audio_info_output, features_output,\n",
        "                transcribe_btn, features_btn\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        transcribe_btn.click(\n",
        "            transcribe_handler,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[transcription_output, audio_info_output]\n",
        "        )\n",
        "\n",
        "        features_btn.click(\n",
        "            features_handler,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[features_output]\n",
        "        )\n",
        "\n",
        "        popular_dropdown.change(\n",
        "            popular_model_selected,\n",
        "            inputs=[popular_dropdown],\n",
        "            outputs=[model_input]\n",
        "        )\n",
        "\n",
        "        # Add examples section\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### üìö Example Models to Try:\n",
        "\n",
        "        - **English ASR:** `facebook/wav2vec2-base-960h` (good for English speech recognition)\n",
        "        - **Multilingual:** `facebook/wav2vec2-large-xlsr-53` (supports 53 languages)\n",
        "        - **Large English:** `facebook/wav2vec2-large-960h-lv60-self` (high accuracy English model)\n",
        "        - **Custom fine-tuned:** Search Hugging Face for domain-specific models\n",
        "\n",
        "        ### üéØ Supported Audio Formats:\n",
        "        WAV, MP3, FLAC, M4A (will be resampled to 16kHz mono)\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Install required packages (run this cell first in Colab)\n",
        "def install_requirements():\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\n",
        "        \"gradio\",\n",
        "        \"transformers\",\n",
        "        \"torch\",\n",
        "        \"torchaudio\",\n",
        "        \"librosa\",\n",
        "        \"datasets\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Uncomment the line below and run to install requirements\n",
        "# install_requirements()\n",
        "\n",
        "# Create and launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "        share=True,  # Creates a public link\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",  # Important for Colab\n",
        "        server_port=7860\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}