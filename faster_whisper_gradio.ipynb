{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gu1hZTmrS-2i"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install faster-whisper ctranslate2 gradio\n",
        "\n",
        "# Import libraries\n",
        "from faster_whisper import WhisperModel\n",
        "import gradio as gr\n",
        "\n",
        "# Global model cache\n",
        "global_model = None\n",
        "\n",
        "def load_model(use_hf_url, hf_url, model_size, device, compute_type):\n",
        "    \"\"\"\n",
        "    Load the Faster-Whisper model with given parameters or a custom Hugging Face URL.\n",
        "    Returns status message.\n",
        "    \"\"\"\n",
        "    global global_model\n",
        "    # Determine model identifier: Hugging Face URL takes priority\n",
        "    model_id = hf_url.strip() if use_hf_url and hf_url.strip() else model_size\n",
        "    # Instantiate the model (downloads CTranslate2 weights if needed)\n",
        "    try:\n",
        "        global_model = WhisperModel(model_id, device=device, compute_type=compute_type)\n",
        "        return f\"✅ Loaded model: {model_id} on {device} ({compute_type})\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Failed to load {model_id}: {e}\"\n",
        "\n",
        "\n",
        "def transcribe(audio, beam_size, language, vad_filter, word_timestamps):\n",
        "    \"\"\"\n",
        "    Transcribe the uploaded audio file with current global_model.\n",
        "    Returns detected language and full transcript.\n",
        "    \"\"\"\n",
        "    if global_model is None:\n",
        "        return \"❌ Model not loaded\", \"\"\n",
        "\n",
        "    segments, info = global_model.transcribe(\n",
        "        audio if isinstance(audio, str) else audio.name,\n",
        "        beam_size=beam_size,\n",
        "        language=language or None,\n",
        "        vad_filter=vad_filter,\n",
        "        word_timestamps=word_timestamps\n",
        "    )\n",
        "\n",
        "    transcript = []\n",
        "    for segment in segments:\n",
        "        transcript.append(f\"[{segment.start:.2f}s -> {segment.end:.2f}] {segment.text}\")\n",
        "    full_text = \"\\n\".join(transcript)\n",
        "    lang_detect = f\"Detected language: {info.language} (p={info.language_probability:.2f})\"\n",
        "    return lang_detect, full_text\n",
        "\n",
        "# Build Gradio UI\n",
        "demo = gr.Blocks()\n",
        "with demo:\n",
        "    gr.Markdown(\"# 🦙 Faster-Whisper Transcription Playground\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            use_hf_url = gr.Checkbox(label=\"Use custom Hugging Face model URL\", value=False)\n",
        "            hf_url = gr.Textbox(label=\"Hugging Face model name or URL\", placeholder=\"e.g. username/my-whisper-ct2\", interactive=True)\n",
        "            model_size = gr.Dropdown(\n",
        "                choices=[\"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\", \"distil-large-v3\"],\n",
        "                value=\"large-v3\",\n",
        "                label=\"Predefined Model Size\",\n",
        "                interactive=True\n",
        "            )\n",
        "            device = gr.Radio(\n",
        "                choices=[\"cuda\", \"cpu\"],\n",
        "                value=\"cuda\",\n",
        "                label=\"Device\"\n",
        "            )\n",
        "            compute_type = gr.Radio(\n",
        "                choices=[\"float16\", \"int8\", \"int8_float16\", \"float32\"],\n",
        "                value=\"float16\",\n",
        "                label=\"Compute Type\"\n",
        "            )\n",
        "            load_btn = gr.Button(\"Load Model\")\n",
        "            load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "            audio_input = gr.Audio(label=\"Upload Audio\", type=\"filepath\")\n",
        "            beam_size = gr.Slider(1, 10, value=5, step=1, label=\"Beam Size\")\n",
        "            language = gr.Textbox(label=\"Force Language (e.g., 'en')\", placeholder=\"Leave empty for auto-detect\")\n",
        "            vad_filter = gr.Checkbox(label=\"VAD Filter (remove silence)\", value=False)\n",
        "            word_timestamps = gr.Checkbox(label=\"Word-level Timestamps\", value=False)\n",
        "            transcribe_btn = gr.Button(\"Transcribe\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            out_lang = gr.Textbox(label=\"Language Detection\")\n",
        "            out_text = gr.Textbox(label=\"Transcript\", lines=20)\n",
        "\n",
        "    # Bind events\n",
        "    load_btn.click(\n",
        "        fn=load_model,\n",
        "        inputs=[use_hf_url, hf_url, model_size, device, compute_type],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "    transcribe_btn.click(\n",
        "        fn=transcribe,\n",
        "        inputs=[audio_input, beam_size, language, vad_filter, word_timestamps],\n",
        "        outputs=[out_lang, out_text]\n",
        "    )\n",
        "\n",
        "# Launch in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, debug=True, share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}