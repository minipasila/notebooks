{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw4uHjG-3HaM"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q gradio\n",
        "!pip install -q librosa soundfile requests\n",
        "!pip install -q sacrebleu\n",
        "!pip install -q omegaconf hydra-core\n",
        "!pip install -q pytorch-lightning\n",
        "!python -m pip install -q \"nemo_toolkit[asr,tts] @ git+https://github.com/NVIDIA/NeMo.git\"\n",
        "\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import tempfile\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import requests\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "# Global variable to store the model\n",
        "model = None\n",
        "device_info = \"\"\n",
        "\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check GPU availability and return device info\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        return f\"✅ GPU Available: {gpu_name} ({gpu_memory:.1f}GB) - {gpu_count} device(s)\"\n",
        "    else:\n",
        "        return \"❌ No GPU available - using CPU (will be very slow)\"\n",
        "\n",
        "def get_gpu_memory_usage():\n",
        "    \"\"\"Get current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "        cached = torch.cuda.memory_reserved(0) / 1024**3\n",
        "        return f\"GPU Memory: {allocated:.1f}GB allocated, {cached:.1f}GB cached\"\n",
        "    return \"CPU mode\"\n",
        "\n",
        "def download_example_files():\n",
        "    \"\"\"Download example audio files for testing\"\"\"\n",
        "    examples_dir = \"example_audio\"\n",
        "    os.makedirs(examples_dir, exist_ok=True)\n",
        "\n",
        "    example_urls = [\n",
        "        (\"https://cdn-media.huggingface.co/speech_samples/sample1.flac\", \"librispeech_sample1.flac\"),\n",
        "        (\"https://cdn-media.huggingface.co/speech_samples/sample2.flac\", \"librispeech_sample2.flac\")\n",
        "    ]\n",
        "\n",
        "    downloaded_files = []\n",
        "\n",
        "    for url, filename in example_urls:\n",
        "        filepath = os.path.join(examples_dir, filename)\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            try:\n",
        "                print(f\"Downloading {filename}...\")\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"✅ Downloaded {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to download {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if os.path.exists(filepath):\n",
        "            downloaded_files.append(filepath)\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load the Canary-Qwen-2.5B model with GPU support\"\"\"\n",
        "    global model, device_info\n",
        "\n",
        "    try:\n",
        "        # Check GPU availability first\n",
        "        device_info = check_gpu_availability()\n",
        "        print(device_info)\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        from nemo.collections.speechlm2.models import SALM\n",
        "        print(\"Loading Canary-Qwen-2.5B model...\")\n",
        "\n",
        "        # Load model - NeMo should automatically use GPU if available\n",
        "        model = SALM.from_pretrained('nvidia/canary-qwen-2.5b')\n",
        "\n",
        "        # Ensure model is on GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "            print(f\"Model moved to GPU: {next(model.parameters()).device}\")\n",
        "        else:\n",
        "            print(\"Model loaded on CPU\")\n",
        "\n",
        "        # Get memory usage after loading\n",
        "        memory_info = get_gpu_memory_usage()\n",
        "        print(f\"Model loaded successfully! {memory_info}\")\n",
        "\n",
        "        return f\"✅ Model loaded successfully!\\n{device_info}\\n{memory_info}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading model: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return f\"❌ {error_msg}\\n{device_info}\"\n",
        "\n",
        "def preprocess_audio(audio_path):\n",
        "    \"\"\"Preprocess audio to meet model requirements (16kHz, mono)\"\"\"\n",
        "    try:\n",
        "        # Load audio file\n",
        "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "        # Create temporary file with processed audio\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
        "        sf.write(temp_file.name, audio, 16000)\n",
        "\n",
        "        return temp_file.name, len(audio) / 16000  # Return path and duration\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error processing audio: {str(e)}\")\n",
        "\n",
        "def split_audio_into_chunks(audio_path: str, chunk_duration: float = 30.0, overlap_duration: float = 2.0) -> List[Tuple[str, float, float]]:\n",
        "    \"\"\"\n",
        "    Split audio into overlapping chunks for processing\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to audio file\n",
        "        chunk_duration: Duration of each chunk in seconds\n",
        "        overlap_duration: Overlap between chunks in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of tuples containing (chunk_path, start_time, end_time)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "        total_duration = len(audio) / sr\n",
        "\n",
        "        if total_duration <= chunk_duration:\n",
        "            # Audio is short enough, return as single chunk\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
        "            sf.write(temp_file.name, audio, sr)\n",
        "            return [(temp_file.name, 0.0, total_duration)]\n",
        "\n",
        "        chunks = []\n",
        "        start_sample = 0\n",
        "        chunk_samples = int(chunk_duration * sr)\n",
        "        overlap_samples = int(overlap_duration * sr)\n",
        "\n",
        "        while start_sample < len(audio):\n",
        "            end_sample = min(start_sample + chunk_samples, len(audio))\n",
        "\n",
        "            # Extract chunk\n",
        "            chunk_audio = audio[start_sample:end_sample]\n",
        "\n",
        "            # Save chunk to temporary file\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
        "            sf.write(temp_file.name, chunk_audio, sr)\n",
        "\n",
        "            start_time = start_sample / sr\n",
        "            end_time = end_sample / sr\n",
        "\n",
        "            chunks.append((temp_file.name, start_time, end_time))\n",
        "\n",
        "            # Move to next chunk with overlap\n",
        "            start_sample += chunk_samples - overlap_samples\n",
        "\n",
        "            # Break if we've reached the end\n",
        "            if end_sample >= len(audio):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error splitting audio: {str(e)}\")\n",
        "\n",
        "def transcribe_chunk(chunk_path: str, max_tokens: int = 128) -> str:\n",
        "    \"\"\"Transcribe a single audio chunk\"\"\"\n",
        "    global model\n",
        "\n",
        "    if model is None:\n",
        "        raise Exception(\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Clear GPU cache before inference\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Transcribe using the model\n",
        "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n",
        "            answer_ids = model.generate(\n",
        "                prompts=[\n",
        "                    [{\"role\": \"user\",\n",
        "                      \"content\": f\"Transcribe the following: {model.audio_locator_tag}\",\n",
        "                      \"audio\": [chunk_path]}]\n",
        "                ],\n",
        "                max_new_tokens=max_tokens,\n",
        "            )\n",
        "\n",
        "        # Get transcription\n",
        "        transcription = model.tokenizer.ids_to_text(answer_ids[0].cpu())\n",
        "\n",
        "        return transcription.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error transcribing chunk: {str(e)}\")\n",
        "\n",
        "def merge_overlapping_transcriptions(transcriptions: List[Tuple[str, float, float]], overlap_duration: float = 2.0) -> str:\n",
        "    \"\"\"\n",
        "    Merge overlapping transcriptions by removing duplicate content in overlap regions\n",
        "\n",
        "    Args:\n",
        "        transcriptions: List of (transcription, start_time, end_time) tuples\n",
        "        overlap_duration: Duration of overlap between chunks\n",
        "\n",
        "    Returns:\n",
        "        Combined transcription\n",
        "    \"\"\"\n",
        "    if not transcriptions:\n",
        "        return \"\"\n",
        "\n",
        "    if len(transcriptions) == 1:\n",
        "        return transcriptions[0][0]\n",
        "\n",
        "    merged_text = transcriptions[0][0]  # Start with first transcription\n",
        "\n",
        "    for i in range(1, len(transcriptions)):\n",
        "        current_text = transcriptions[i][0]\n",
        "\n",
        "        # Simple approach: try to find common words at the end of previous and start of current\n",
        "        prev_words = merged_text.split()\n",
        "        curr_words = current_text.split()\n",
        "\n",
        "        # Look for overlap in the last few words of previous and first few words of current\n",
        "        max_overlap_words = min(len(prev_words), len(curr_words), 20)  # Check up to 20 words\n",
        "\n",
        "        best_overlap = 0\n",
        "        for j in range(1, max_overlap_words + 1):\n",
        "            if prev_words[-j:] == curr_words[:j]:\n",
        "                best_overlap = j\n",
        "\n",
        "        if best_overlap > 0:\n",
        "            # Remove overlapping words from current transcription\n",
        "            merged_text += \" \" + \" \".join(curr_words[best_overlap:])\n",
        "        else:\n",
        "            # No clear overlap found, just concatenate with space\n",
        "            merged_text += \" \" + current_text\n",
        "\n",
        "    return merged_text.strip()\n",
        "\n",
        "def transcribe_audio(audio_file, max_tokens=128):\n",
        "    \"\"\"Transcribe audio using Canary-Qwen-2.5B\"\"\"\n",
        "    global model\n",
        "\n",
        "    if model is None:\n",
        "        return \"❌ Model not loaded. Please load the model first.\"\n",
        "\n",
        "    if audio_file is None:\n",
        "        return \"❌ Please upload an audio file.\"\n",
        "\n",
        "    try:\n",
        "        # Preprocess audio\n",
        "        processed_audio_path, duration = preprocess_audio(audio_file)\n",
        "\n",
        "        # Check duration (model supports up to 40s)\n",
        "        if duration > 40:\n",
        "            warning_msg = f\"⚠️ Audio duration ({duration:.1f}s) exceeds recommended limit of 40s. Results may be degraded.\"\n",
        "            print(warning_msg)\n",
        "\n",
        "        # Clear GPU cache before inference\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"Starting transcription on device: {next(model.parameters()).device}\")\n",
        "\n",
        "        # Transcribe using the model\n",
        "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n",
        "            answer_ids = model.generate(\n",
        "                prompts=[\n",
        "                    [{\"role\": \"user\",\n",
        "                      \"content\": f\"Transcribe the following: {model.audio_locator_tag}\",\n",
        "                      \"audio\": [processed_audio_path]}]\n",
        "                ],\n",
        "                max_new_tokens=max_tokens,\n",
        "            )\n",
        "\n",
        "        # Get transcription\n",
        "        transcription = model.tokenizer.ids_to_text(answer_ids[0].cpu())\n",
        "\n",
        "        # Clean up temporary file\n",
        "        os.unlink(processed_audio_path)\n",
        "\n",
        "        # Get current memory usage\n",
        "        memory_info = get_gpu_memory_usage()\n",
        "\n",
        "        result = f\"🎯 **Transcription:** {transcription}\\n\\n📊 **Duration:** {duration:.1f}s\\n💾 **{memory_info}**\"\n",
        "\n",
        "        if duration > 40:\n",
        "            result = f\"⚠️ Audio duration ({duration:.1f}s) exceeds recommended limit of 40s.\\n\\n\" + result\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error during transcription: {str(e)}\"\n",
        "\n",
        "def transcribe_long_audio(audio_file, chunk_duration=30, overlap_duration=2, max_tokens=128, progress=gr.Progress()):\n",
        "    \"\"\"Transcribe long audio by splitting into chunks and processing with streaming\"\"\"\n",
        "    global model\n",
        "\n",
        "    if model is None:\n",
        "        return \"❌ Model not loaded. Please load the model first.\"\n",
        "\n",
        "    if audio_file is None:\n",
        "        return \"❌ Please upload an audio file.\"\n",
        "\n",
        "    try:\n",
        "        # Preprocess audio to get duration\n",
        "        processed_audio_path, total_duration = preprocess_audio(audio_file)\n",
        "\n",
        "        progress(0, desc=f\"Preparing audio ({total_duration:.1f}s)...\")\n",
        "\n",
        "        # Split audio into chunks\n",
        "        chunks = split_audio_into_chunks(\n",
        "            processed_audio_path,\n",
        "            chunk_duration=chunk_duration,\n",
        "            overlap_duration=overlap_duration\n",
        "        )\n",
        "\n",
        "        progress(0.1, desc=f\"Split into {len(chunks)} chunks...\")\n",
        "\n",
        "        # Transcribe each chunk\n",
        "        transcriptions = []\n",
        "\n",
        "        for i, (chunk_path, start_time, end_time) in enumerate(chunks):\n",
        "            progress_val = 0.1 + (i / len(chunks)) * 0.8\n",
        "            progress(progress_val, desc=f\"Transcribing chunk {i+1}/{len(chunks)} ({start_time:.1f}s-{end_time:.1f}s)...\")\n",
        "\n",
        "            try:\n",
        "                transcription = transcribe_chunk(chunk_path, max_tokens)\n",
        "                transcriptions.append((transcription, start_time, end_time))\n",
        "\n",
        "                # Clean up chunk file\n",
        "                os.unlink(chunk_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error transcribing chunk {i+1}: {e}\")\n",
        "                transcriptions.append((f\"[Error in chunk {i+1}]\", start_time, end_time))\n",
        "\n",
        "        progress(0.9, desc=\"Merging transcriptions...\")\n",
        "\n",
        "        # Merge overlapping transcriptions\n",
        "        final_transcription = merge_overlapping_transcriptions(transcriptions, overlap_duration)\n",
        "\n",
        "        # Clean up main processed audio file\n",
        "        os.unlink(processed_audio_path)\n",
        "\n",
        "        # Get memory usage\n",
        "        memory_info = get_gpu_memory_usage()\n",
        "\n",
        "        progress(1.0, desc=\"Complete!\")\n",
        "\n",
        "        # Format result with detailed info\n",
        "        result = f\"\"\"🎯 **Final Transcription:**\n",
        "{final_transcription}\n",
        "\n",
        "📊 **Processing Details:**\n",
        "- Total Duration: {total_duration:.1f}s\n",
        "- Number of Chunks: {len(chunks)}\n",
        "- Chunk Duration: {chunk_duration}s\n",
        "- Overlap Duration: {overlap_duration}s\n",
        "- Successful Chunks: {len([t for t in transcriptions if not t[0].startswith('[Error')])}\n",
        "\n",
        "💾 **{memory_info}**\"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error during long audio transcription: {str(e)}\"\n",
        "\n",
        "def post_process_transcript(transcript, user_prompt, max_tokens=512):\n",
        "    \"\"\"Use LLM mode for post-processing the transcript\"\"\"\n",
        "    global model\n",
        "\n",
        "    if model is None:\n",
        "        return \"❌ Model not loaded. Please load the model first.\"\n",
        "\n",
        "    if not transcript.strip():\n",
        "        return \"❌ Please provide a transcript to process.\"\n",
        "\n",
        "    if not user_prompt.strip():\n",
        "        return \"❌ Please provide a prompt for post-processing.\"\n",
        "\n",
        "    try:\n",
        "        # Clear GPU cache before inference\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"Starting LLM processing on device: {next(model.parameters()).device}\")\n",
        "\n",
        "        # Use LLM mode (disable adapter to use base LLM capabilities)\n",
        "        with model.llm.disable_adapter():\n",
        "            with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n",
        "                answer_ids = model.generate(\n",
        "                    prompts=[[{\"role\": \"user\",\n",
        "                              \"content\": f\"{user_prompt}\\n\\n{transcript}\"}]],\n",
        "                    max_new_tokens=max_tokens,\n",
        "                )\n",
        "\n",
        "        response = model.tokenizer.ids_to_text(answer_ids[0].cpu())\n",
        "\n",
        "        # Get current memory usage\n",
        "        memory_info = get_gpu_memory_usage()\n",
        "\n",
        "        return f\"🤖 **Response:** {response}\\n\\n💾 **{memory_info}**\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error during post-processing: {str(e)}\"\n",
        "\n",
        "# Check GPU availability at startup\n",
        "print(\"=== GPU Information ===\")\n",
        "startup_gpu_info = check_gpu_availability()\n",
        "print(startup_gpu_info)\n",
        "\n",
        "# Download example files\n",
        "print(\"\\n=== Downloading Example Files ===\")\n",
        "example_files = download_example_files()\n",
        "\n",
        "# Create example prompts for post-processing\n",
        "example_prompts = [\n",
        "    \"Summarize this transcript in 2-3 sentences:\",\n",
        "    \"Extract the main topics discussed in this transcript:\",\n",
        "    \"What are the key action items mentioned in this transcript?\",\n",
        "    \"Identify any questions asked in this transcript:\",\n",
        "    \"Correct any grammar or formatting issues in this transcript:\"\n",
        "]\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"Nvidia Canary-Qwen-2.5B Speech Recognition\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(f\"\"\"\n",
        "    # 🎤 Nvidia Canary-Qwen-2.5B Speech Recognition\n",
        "\n",
        "    This interface uses Nvidia's state-of-the-art Canary-Qwen-2.5B model for English speech recognition and transcript post-processing.\n",
        "\n",
        "    **System Info:** {startup_gpu_info}\n",
        "\n",
        "    **Features:**\n",
        "    - 🎯 High-accuracy English speech transcription\n",
        "    - 📝 Automatic punctuation and capitalization\n",
        "    - 🚀 **NEW:** Streaming support for long audio files\n",
        "    - 🤖 LLM-powered transcript post-processing\n",
        "    - ⚡ Fast inference (418 RTFx on GPU)\n",
        "    - 🚀 GPU acceleration enabled\n",
        "\n",
        "    **Supported formats:** WAV, FLAC, MP3, M4A (automatically converted to 16kHz mono)\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"🎤 Speech Transcription\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Model loading section\n",
        "                gr.Markdown(\"### 1. Load Model\")\n",
        "                load_btn = gr.Button(\"🔄 Load Canary-Qwen-2.5B Model\", variant=\"primary\")\n",
        "                load_status = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
        "\n",
        "                gr.Markdown(\"### 2. Upload Audio\")\n",
        "                audio_input = gr.Audio(\n",
        "                    label=\"Upload Audio File (≤40s recommended)\",\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### 3. Transcription Settings\")\n",
        "                max_tokens = gr.Slider(\n",
        "                    minimum=32,\n",
        "                    maximum=512,\n",
        "                    value=128,\n",
        "                    step=32,\n",
        "                    label=\"Max Tokens\"\n",
        "                )\n",
        "\n",
        "                transcribe_btn = gr.Button(\"🎯 Transcribe Audio\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### 📄 Transcription Result\")\n",
        "                transcription_output = gr.Textbox(\n",
        "                    label=\"Transcription\",\n",
        "                    lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        # Example audio files section (only show if files were downloaded successfully)\n",
        "        if example_files:\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 🎵 Try with Example Audio\n",
        "            Click on one of the example files below to test the transcription.\n",
        "            \"\"\")\n",
        "\n",
        "            examples = gr.Examples(\n",
        "                examples=[[file] for file in example_files],\n",
        "                inputs=[audio_input],\n",
        "                label=\"LibriSpeech Example Files\"\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"🎬 Long Audio Streaming\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🚀 Stream Long Audio Files\n",
        "        Process audio files of any length by automatically splitting them into chunks with smart overlap handling.\n",
        "        Perfect for meetings, lectures, podcasts, and long-form content.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Upload Long Audio\")\n",
        "                long_audio_input = gr.Audio(\n",
        "                    label=\"Upload Audio File (any length)\",\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### Streaming Settings\")\n",
        "                with gr.Row():\n",
        "                    chunk_duration = gr.Slider(\n",
        "                        minimum=15,\n",
        "                        maximum=35,\n",
        "                        value=30,\n",
        "                        step=5,\n",
        "                        label=\"Chunk Duration (seconds)\"\n",
        "                    )\n",
        "                    overlap_duration = gr.Slider(\n",
        "                        minimum=1,\n",
        "                        maximum=5,\n",
        "                        value=2,\n",
        "                        step=1,\n",
        "                        label=\"Overlap Duration (seconds)\"\n",
        "                    )\n",
        "\n",
        "                streaming_max_tokens = gr.Slider(\n",
        "                    minimum=32,\n",
        "                    maximum=512,\n",
        "                    value=128,\n",
        "                    step=32,\n",
        "                    label=\"Max Tokens per Chunk\"\n",
        "                )\n",
        "\n",
        "                stream_btn = gr.Button(\"🎬 Start Streaming Transcription\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### 📺 Streaming Results\")\n",
        "                streaming_output = gr.Textbox(\n",
        "                    label=\"Live Transcription\",\n",
        "                    lines=15,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **How it works:**\n",
        "        1. Audio is split into overlapping chunks (default: 30s chunks with 2s overlap)\n",
        "        2. Each chunk is transcribed independently\n",
        "        3. Overlapping regions are intelligently merged to avoid duplicate words\n",
        "        4. Progress is shown in real-time\n",
        "\n",
        "        **Optimal settings:**\n",
        "        - **Chunk Duration:** 30s (balances accuracy and processing speed)\n",
        "        - **Overlap:** 2s (prevents word cutoffs at boundaries)\n",
        "        - Use shorter chunks for very noisy audio or multiple speakers\n",
        "        \"\"\")\n",
        "\n",
        "    with gr.Tab(\"🤖 Transcript Post-Processing\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### LLM Mode - Post-Process Your Transcripts\n",
        "        Use the underlying LLM capabilities to analyze, summarize, or process your transcripts.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                transcript_input = gr.Textbox(\n",
        "                    label=\"Transcript\",\n",
        "                    lines=6,\n",
        "                    placeholder=\"Paste your transcript here or use the output from the transcription tabs...\"\n",
        "                )\n",
        "\n",
        "                user_prompt = gr.Dropdown(\n",
        "                    choices=example_prompts,\n",
        "                    label=\"Choose a prompt or type your own\",\n",
        "                    allow_custom_value=True,\n",
        "                    value=example_prompts[0]\n",
        "                )\n",
        "\n",
        "                llm_max_tokens = gr.Slider(\n",
        "                    minimum=64,\n",
        "                    maximum=1024,\n",
        "                    value=512,\n",
        "                    step=64,\n",
        "                    label=\"Max Response Tokens\"\n",
        "                )\n",
        "\n",
        "                process_btn = gr.Button(\"🤖 Process Transcript\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                llm_output = gr.Textbox(\n",
        "                    label=\"LLM Response\",\n",
        "                    lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "    with gr.Tab(\"ℹ️ Model Information\"):\n",
        "        gr.Markdown(f\"\"\"\n",
        "        ### About Canary-Qwen-2.5B\n",
        "\n",
        "        **System Information:**\n",
        "        {startup_gpu_info}\n",
        "\n",
        "        **Model Details:**\n",
        "        - 🔢 **Parameters:** 2.5 billion\n",
        "        - 🌍 **Language:** English only\n",
        "        - ⚡ **Speed:** 418 RTFx (Real-Time Factor on GPU)\n",
        "        - 🎯 **Architecture:** Speech-Augmented Language Model (SALM)\n",
        "        - 📊 **Performance:** State-of-the-art on multiple English benchmarks\n",
        "\n",
        "        **Capabilities:**\n",
        "        - **ASR Mode:** High-accuracy speech transcription with punctuation and capitalization\n",
        "        - **Streaming Mode:** Process long audio files with intelligent chunking\n",
        "        - **LLM Mode:** Text-only processing for summarization, Q&A, and analysis\n",
        "\n",
        "        **Limitations:**\n",
        "        - Maximum audio duration per chunk: 40 seconds (handled automatically in streaming mode)\n",
        "        - English language only\n",
        "        - Requires 16kHz mono audio (automatically handled by this interface)\n",
        "\n",
        "        **Training Data:**\n",
        "        - 234.5k hours of English speech data\n",
        "        - Public datasets including LibriSpeech, Common Voice, YouTube Commons, and more\n",
        "\n",
        "        **License:** CC-BY-4.0\n",
        "\n",
        "        ### Performance Benchmarks\n",
        "\n",
        "        **WER (Word Error Rate) on OpenASR Leaderboard:**\n",
        "        - LibriSpeech Clean: 1.60%\n",
        "        - LibriSpeech Other: 3.10%\n",
        "        - AMI Meetings: 10.18%\n",
        "        - GigaSpeech: 9.41%\n",
        "        - Earnings-22: 10.42%\n",
        "        - SPGISpeech: 1.90%\n",
        "        - Tedlium: 2.72%\n",
        "        - VoxPopuli: 5.66%\n",
        "\n",
        "        ### Streaming Features\n",
        "\n",
        "        **New in this version:**\n",
        "        - 🎬 **Long Audio Support:** Process audio files of any length\n",
        "        - 🔄 **Intelligent Chunking:** Automatic splitting with overlap handling\n",
        "        - 📊 **Progress Tracking:** Real-time progress updates\n",
        "        - 🧠 **Smart Merging:** Removes duplicate words from overlapping segments\n",
        "        - ⚙️ **Configurable Settings:** Adjust chunk size and overlap for optimal results\n",
        "\n",
        "        ### GPU Requirements\n",
        "        - **Recommended:** T4, V100, A100, or better\n",
        "        - **Memory:** ~6-8GB GPU memory for inference\n",
        "        - **Speed:** ~418x real-time on modern GPUs\n",
        "        - **Streaming:** Can process hours of audio with consistent performance\n",
        "        \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    load_btn.click(\n",
        "        fn=load_model,\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    transcribe_btn.click(\n",
        "        fn=transcribe_audio,\n",
        "        inputs=[audio_input, max_tokens],\n",
        "        outputs=[transcription_output]\n",
        "    )\n",
        "\n",
        "    stream_btn.click(\n",
        "        fn=transcribe_long_audio,\n",
        "        inputs=[long_audio_input, chunk_duration, overlap_duration, streaming_max_tokens],\n",
        "        outputs=[streaming_output]\n",
        "    )\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=post_process_transcript,\n",
        "        inputs=[transcript_input, user_prompt, llm_max_tokens],\n",
        "        outputs=[llm_output]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )"
      ]
    }
  ]
}